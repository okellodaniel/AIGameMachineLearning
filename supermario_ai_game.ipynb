{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SetUp SuperMario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n        Libraries installed\\n==================================\\n1. pip install gym-super-mario-bros  => (\"Allows us to play Mario in Python\") {https://pypi.org/project/gym-super-mario-bros/}\\n2. pip install nes-py {https://pypi.org/project/nes-py/}\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''SetUp of Super Mario'''\n",
    "\n",
    "'''\n",
    "\n",
    "        Libraries installed\n",
    "==================================\n",
    "1. pip install gym-super-mario-bros  => (\"Allows us to play Mario in Python\") {https://pypi.org/project/gym-super-mario-bros/}\n",
    "2. pip install nes-py {https://pypi.org/project/nes-py/}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing Dependencies'''\n",
    "# 1. Import Super Mario\n",
    "# 2. Import the Joypad Wrapper (\"Wraps the Movement of our model ==> Joypadspace wrapper makes it easier for our AI to learn how to play\")\n",
    "# 3. Import the Simplified Controls\n",
    "\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NOOP'],\n",
       " ['right'],\n",
       " ['right', 'A'],\n",
       " ['right', 'B'],\n",
       " ['right', 'A', 'B'],\n",
       " ['A'],\n",
       " ['left']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIMPLE_MOVEMENT # Shows us the Directions the AI is limited to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SetUp the Game\n",
    "# Environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env,SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 256, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # (Try with or without Wrapper) [Lists the Number of Actions]\n",
    "# Check the Observation Space\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag set to True, allows to start the game\n",
    "done = True\n",
    "\n",
    " # Loop through each Frame in the game\n",
    "for step in range(100000):\n",
    "    if done:\n",
    "        # Start the game\n",
    "        env.reset() # Allows us to restart the game\n",
    "\n",
    "    # Pass through an action to the game  && Do random actions\n",
    "    state,reward, done, info = env.step(env.action_space.sample())\n",
    "    \n",
    "    # Show game on the Screen\n",
    "    env.render()\n",
    "\n",
    "# Close the Game\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n- State => (env.step(1)[0] (Particular frame)\\n- Reward => (env.step(1)[1] (Reward obtained)\\n- Done => (env.step(1)[0] (Whether we are dead or not)\\n- Info => (env.step(1)[0] (Basic Info)\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Explaining the State, Reward, Done && Info'''\n",
    "# env.reset()\n",
    "# state = env.reset()\n",
    "# state.shape # A state is basically a Frame from the game\n",
    "\n",
    "# On taking a Step, 4 values are returned\n",
    "''' \n",
    "- State => (env.step(1)[0] (Particular frame)\n",
    "- Reward => (env.step(1)[1] (Reward obtained)\n",
    "- Done => (env.step(1)[0] (Whether we are dead or not)\n",
    "- Info => (env.step(1)[0] (Basic Info)\n",
    "\n",
    "'''\n",
    "# env.step(1)[0]\n",
    "# env.step(1)[1]\n",
    "# env.step(1)[2]\n",
    "# env.step(1)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pre-Processing the Game Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     \\n    Pre-processing steps\\n============================\\n-  Gray Scaling (Cuts down data the AI has to learn from)\\n-  Frame Stacking (Gives our model memory, Helps determine Trajectory and Velocity)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''     \n",
    "    Pre-processing steps\n",
    "============================\n",
    "-  Gray Scaling (Cuts down data the AI has to learn from)\n",
    "-  Frame Stacking (Gives our model memory, Helps determine Trajectory and Velocity)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies / Libraries\n",
    "'''\n",
    "1. Import Frame Stacker Wrapper and Grayscaling Wrapper\n",
    "2. Import Vectorization Wrappers\n",
    "3. Import MatplotLib  => Will Show us the impact of Frame Stacking\n",
    "\n",
    "=>  Stablebaselines3 is a reiforcement library (https://stable-baselines3.readthedocs.io/en/master/)\n",
    "    - Gives algorithms that are used to train your AI model\n",
    "'''\n",
    "\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1 . Create the Base Environment\n",
    "2 . Simplify the Controls\n",
    "3 . GrayScale Environment\n",
    "4 . Wrap inside the Dummy environment (=> Dummy Stable Baseline Vectorization environment ) {==> Changes the State of our Data}\n",
    "5 . Stack the Frames\n",
    "\n",
    "'''\n",
    "#1\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "\n",
    "#2\n",
    "env =JoypadSpace(env,SIMPLE_MOVEMENT) \n",
    "\n",
    "#3\n",
    "env = GrayScaleObservation(env,keep_dim=True)\n",
    "\n",
    "#4\n",
    "\n",
    "env = DummyVecEnv([lambda:env])\n",
    "\n",
    "#5 \n",
    "env = VecFrameStack(env,4,channels_order='last') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# state.shape\n",
    "\n",
    "# # # '''\n",
    "# # # Without GrayScaling, you get (240,256,3) => 240 * 256 pixels, and 3 channels ==> Colored Picture\n",
    "# # # '''\n",
    "\n",
    "# # #Use MatplotLib to show the game Frame\n",
    "\n",
    "# # plt.imshow(state)\n",
    "\n",
    "# # '''\n",
    "# #  After passing data through a Dummy Environment\n",
    "# # '''\n",
    "# # plt.imshow(state[0])\n",
    "# plt.imshow(state[0])\n",
    "# env = VecFrameStack(env,4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state,reward, done, info = env.step([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,16))\n",
    "# for i in range(state.shape[3]):\n",
    "#     plt.subplot(1,4,i+1)\n",
    "#     plt.imshow(state[0][:,:,i])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Import OS => For file Path management (Determine Where to save our models)\\n2. Import PPO (Proximal Policy Optimisation Algorithm) from                         =====> https://stable-baselines3.readthedocs.io/en/master/guide/install.html\\n    baseline to facilitate - Reinforcement Learning(1. Agent   3. Environment\\n                                                    2. Action  4. Reward)       \\n3. Import Callbacks to save models after a certain number of steps                                              \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Dependencies'''\n",
    "'''\n",
    "1. Import OS => For file Path management (Determine Where to save our models)\n",
    "2. Import PPO (Proximal Policy Optimisation Algorithm) from                         =====> https://stable-baselines3.readthedocs.io/en/master/guide/install.html\n",
    "    baseline to facilitate - Reinforcement Learning(1. Agent   3. Environment\n",
    "                                                    2. Action  4. Reward)       \n",
    "3. Import Callbacks to save models after a certain number of steps                                              \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__ (self,check_freq,save_path,verbose=1):\n",
    "        super(TrainAndLoggingCallback,self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "    \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path,exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path,'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for saving the trained models and Logs\n",
    "\n",
    "CHECKPOINT_DIR = './train/'\n",
    "LOG_DIR = './logs/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Callback\n",
    "\n",
    "callback = TrainAndLoggingCallback(check_freq=10000,save_path=CHECKPOINT_DIR,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Setup the Model(PPO Model)\n",
    "# CnnPolicy => Convolutional Neural Network Policy {Deep learning neural network that processes Images faster} other neural networks include RNNs, LSTMs, GRUs, mlp\n",
    "# verbose => 1 => Prints the training progress\n",
    "# learning_rate => Learning Rate\n",
    "# n_steps => Number of Steps\n",
    "\n",
    "model = PPO('CnnPolicy', env, verbose = 1, tensorboard_log=LOG_DIR, learning_rate=0.000001, n_steps=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/PPO_1\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 7   |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 66  |\n",
      "|    total_timesteps | 512 |\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the AI model. This is ehere the model starts to learn\n",
    "''' ==> More like how many frames our AI is going to see \n",
    "- In this case for every frame, our AI  will see 1000000 frames'''\n",
    "\n",
    "model.learn(total_timesteps=1000000,callback=callback)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
